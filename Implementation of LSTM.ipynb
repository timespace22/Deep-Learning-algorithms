{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66388d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# nlp library of Pytorch\n",
    "from torchtext import data\n",
    "#from torchtext.legacy import data\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "SEED = 2023\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cuda.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317687d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.read_csv('.dataset/sms_spam.csv')\n",
    "data_.head()\n",
    "data_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a46d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field is a normal column \n",
    "# LabelField is the label column.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenizer,batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"type\",LABEL),('text',TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb93699",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.TabularDataset(path=\".dataset/sms_spam.csv\",\n",
    "                                    format=\"csv\",\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True\n",
    "                                   )\n",
    "\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc923ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# train and validation splitting\n",
    "train_data,valid_data = training_data.split(split_ratio=0.75,\n",
    "                                            random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocabularies => (Token to integer)\n",
    "fields[1][1].build_vocab(train_data)\n",
    "fields[0][1].build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of text vocab:\",len(fields[1][1].vocab))\n",
    "print(\"Size of label vocab:\",len(fields[0][1].vocab))\n",
    "fields[1][1].vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# We'll create iterators to get batches of data when we want to use them\n",
    "\"\"\"\n",
    "This BucketIterator batches the similar length of samples and reduces the need of \n",
    "padding tokens. This makes our future model more stable\n",
    "\n",
    "\"\"\"\n",
    "train_iterator,validation_iterator = data.BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Sort key is how to sort the samples\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b707f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "        \n",
    "        super(LSTMNet,self).__init__()\n",
    "        # Implement the architecture of an LSTM network\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.output_dim=output_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout_layer=nn.Dropout(p = dropout)\n",
    "        \n",
    "        # 1. Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # 2. LSTM layer process the vector sequences \n",
    "        \n",
    "        #zt\n",
    "        self.Wz1 = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_dim))\n",
    "        self.Wz2 = nn.Parameter(torch.Tensor(self.hidden_dim, self.hidden_dim))\n",
    "        \n",
    "        #rt\n",
    "        self.Wr1 = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_dim))\n",
    "        self.Wr2 = nn.Parameter(torch.Tensor(self.hidden_dim, self.hidden_dim))\n",
    "        \n",
    "        #ht\n",
    "        self.W1 = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_dim))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hidden_dim, self.hidden_dim))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        # 3. Dense layer to predict \n",
    "        self.dense_layer = nn.Linear(self.hidden_dim , self.output_dim)\n",
    "        \n",
    "        # 4. Prediction activation function (you can choose your own activate function e.g., ReLU, Sigmoid, Tanh)\n",
    "        self.activation_layer = nn.Sigmoid()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "           \n",
    "    def forward(self,text,text_lengths):\n",
    "        embedded_output = self.dropout_layer(self.embedding_layer(text))\n",
    "        size_array = embedded_output.size()\n",
    "        batch_size = size_array[0]\n",
    "        seq_size = size_array[1]\n",
    "        \n",
    "        lstm_seq = []\n",
    "        ht = torch.zeros(batch_size, self.hidden_dim).to(embedded_output.device)\n",
    "        \n",
    "        for word_num in range(seq_size):\n",
    "            xt = embedded_output[:, word_num, :]\n",
    "            \n",
    "            for _ in range(self.n_layers):\n",
    "                zt = torch.sigmoid(xt @ self.Wz1 + ht @ self.Wz2)\n",
    "                rt = torch.sigmoid(xt @ self.Wr1 + ht @ self.Wr2)\n",
    "                ht_ = torch.tanh(xt @ self.W1 + (rt*ht) @ self.W2)\n",
    "                ht = (1-zt)*ht + zt*ht_\n",
    "            \n",
    "        lstm_seq.append(ht.unsqueeze(0))\n",
    "        lstm_out = lstm_seq[-1][:][:]\n",
    "\n",
    "        lstm_out = lstm_out.transpose(0, 1).contiguous()\n",
    "        \n",
    "        dense_output = self.dense_layer(lstm_out)\n",
    "        \n",
    "        output = self.activation_layer(dense_output)\n",
    "        \n",
    "        return output           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HIDDEN_NODES = 64\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 4\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ff74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(SIZE_OF_VOCAB,\n",
    "                EMBEDDING_DIM,\n",
    "                NUM_HIDDEN_NODES,\n",
    "                NUM_OUTPUT_NODES,\n",
    "                NUM_LAYERS,\n",
    "                BIDIRECTION,\n",
    "                DROPOUT\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15faa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # cleaning the cache of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text,text_lengths = batch.text\n",
    "        \n",
    "        # forward propagation and squeezing\n",
    "        predictions = model(text,text_lengths).squeeze()\n",
    "        \n",
    "        # computing loss / backward propagation\n",
    "        loss = criterion(predictions,batch.type)\n",
    "        loss.backward()\n",
    "        \n",
    "        # accuracy\n",
    "        acc = binary_accuracy(predictions,batch.type)\n",
    "        \n",
    "        # updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # It'll return the means of loss and accuracy\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    # deactivate the dropouts\n",
    "    model.eval()\n",
    "    \n",
    "    # Sets require_grad flat False\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text,text_lengths).squeeze()\n",
    "              \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.type)\n",
    "            acc = binary_accuracy(predictions, batch.type)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUMBER = 15\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    \n",
    "    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n",
    "    \n",
    "    valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n",
    "    \n",
    "    print(epoch)\n",
    "    \n",
    "    # Showing statistics\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
